<div align="center">

# SimCLR

SimCLR implementation with scripts for: self-supervised encoder pretraining, linear evaluation, fine-tuning and transfer learning across multiple datasets.

</div>

## 1. Overview
This repository contains:
- `train_encoder.py`: Pretraining of the encoder.
- `linear_evaluation.py`: Freezes the encoder and trains a linear classifier.
- `transfer_learning.py`: If done in the same dataset as the encoder, a fine-tuning is performed. If not, a transfer learning is performed.
- `configs/*.yaml`: Configs.
- `src/`: Source code.
- `tools/create_datasets.py`: Script to download datasets.

## 2. Train encoder

In `configs/train_encoder_imagenet` there is an example of pretraining of the encoder where

<pre>
output_path: Output for the results
datasets_path: Datasets folder generated by tools/create_datasets.py
train_datasets: ["dataset"] where dataset is where the encoder will be pre-trained
batch_size: Batch size
num_epochs: Number of epochs
lr: Learning rate
weight_decay: Weight decay
num_workers: num_workers for the dataloader
prefetch_factor: prefetch_factor for the dataloader
model: Model to train the encoder. It can be 'resnet18' or 'resnet50'
transform_resize: [n, n] where n is the number of pixels to resize the images
projection_head_mode: Projection head mode. It can be non-linear, linear or none
temperature: Temperature
projection_dim: Dimension of the latent space of the projection head
warmup_epochs: Warmup epochs
pin_memory: pin_memory for the dataloader
use_checkpoint: True if you want to trade memory efficiency for time eficciency, else False
pretrained: True if you want to use pre-trained weights on model, else False
use_val_subset: True if you want to separate some training data for validation. Util if you want to test hyperparameters
optimizer: Optimizer. It can be 'lars' or 'adamw'
</pre>

The datasets on the <train_datasets> field can be

<pre>
cifar10
cifar100
imagenet
dtd
fgvc-aircraft
flowers-102
food-101
oxford-pets
stanford-cars
tiny-imagenet
caltech-101
</pre>

To run the encoder pre-training, you can use

```
python3 train_encoder.py --config <path_to_config> --gpu <gpu_index>
```

A folder like this will be created:

<pre>
output_path/
└── execution1/
    └── train_encoder/
        ├── config.yaml
        ├── figs/
        ├── log.txt
        ├── models/
        └── normalization,json
</pre>

If you run the same input, a new folder `execution2` will be created.

## 3. Linear Evaluation, Fine-tuning and Transfer Learning

A Linear Evaluation, Fine-tuning or Transfer Learning config is shown like:

<pre>
train_datasets: ["dataset"] where dataset is where the linear_evaluation will happen
batch_size: Batch size
label_fractions: [label_fraction]. Percentage of data that will be used to train the linear classifier. It can be 1.0 (standard) or lower.
num_epochs: [num_epochs]
lr: [learning_rate]
weight_decay: [weight_decay]
num_workers: num_workers for the dataloader
prefetch_factor: prefetch_factor for the dataloader
pin_memory: pin_memory for the dataloader
use_checkpoint: True if you want to trade memory efficiency for time eficciency, else False
pretrained: True if you want to use pre-trained weights on model, else False
use_val_subset: True if you want to separate some training data for validation. Util if you want to test hyperparameters
</pre>

Note that `label_fractions`, `num_epochs`, `lr` and `weight_decay` are lists. If you put two elements in this list, the two configurations will be performed sequentialy, creating a folder for each one. All these parameters have to have the same length

To perform Linear Evaluation on the pre-trained encoder, you can run:

```
python3 linear_evaluation.py --config <path_to_config> --gpu <gpu_index> --train_dir <output_path field from train_encoder config>
```

Examples of linear_evaluation configs can be found at `configs/linear_evaluation*.yaml`

To perform Fine-tuning or Transfer Learning on the pre-trained encoder, you can run:

```
python3 transfer_learning.py --config <path_to_config> --gpu <gpu_index> --train_dir <output_path field from train_encoder config>
```

Note that if you want to perform fine-tuning, the dataset in fine-tuning config must be the same dataset from the encoder pre-training dataset. If you use others, transfer_learning will be performed.

Folders like this will be created:

<pre>
└── train_encoder_imagenet
    └── execution_1
        ├── train_encoder
        │   ├── config.yaml
        │   ├── figs/
        │   ├── log.txt
        │   ├── models/
        │   └── normalization.json
        ├── linear_evaluation_imagenet
        │   └── lf_<label_fraction>_ne_<num_epochs>_lr_<lr>_wd_<weight_decay>
        │       ├── config.yaml
        │       ├── encoder_config.yaml
        │       ├── figs/
        │       ├── log.txt
        │       ├── models/
        │       └── results_lb_<label_fraction>_ne_<num_epochs>_lr_<lr>_wd_<weight_decay>
        └── transfer_learning_imagenet
            └── lf_<label_fraction>_ne_<num_epochs>_lr_<lr>_wd_<weight_decay>
                ├── config.yaml
                ├── encoder_config.yaml
                ├── figs/
                ├── log.txt
                ├── models/
                └── results_lb_<label_fraction>_ne_<num_epochs>_lr_<lr>_wd_<weight_decay>
</pre>

## 3. Linear Evaluation, Fine-tuning and Transfer Learning with pre-trained weights from SimCLR

You can evaluate the models from SimCLR papers too

You can download the weights from the [SimCLR repository](https://github.com/google-research/simclr). Then, you can transform the weights to pytorch with [this](https://github.com/tonylins/simclr-converter) repository

For Linear Evaluation, you can run:

```
python3 liner_evaluation.py --config <path_to_config> --gpu <gpu_index> --pretrained_encoder <path_to_model> --output_dir <output_dir> --datasets_folder_path <same as datasets_path from train_encoder config>
```

For Fine-tuning or Transfer Learning, you can run:

```
python3 transfer_learning.py --config <path_to_config> --gpu <gpu_index> --pretrained_encoder <path_to_model> --output_dir <output_dir> --datasets_folder_path <same as datasets_path from train_encoder config>
```

## 4. Results

This repository nearly reproduces SimCLR results. The difference is probably because of the Batch Size and de Learning Rate used for both Linear Evaluation and Transfer Learning assessments. In the original paper, the Linear Evaluation was performed under a Batch Size of 4096 with Learning Rate of 1.6. Here, the Linear Evaluation was performed ander a Batch Size of 512 with Learning Rate of 0.2. The same occurs with the Fine-tuning, but with a learning rate of 0.1 instead of the original 0.8. The models can be found [here](https://drive.google.com/drive/folders/1XikVzuvVJq-RFh1TpekWgbp4ya-Z0J1I?usp=sharing)

| Dataset | Architecture | Epochs | Batch Size | Optimizer | Top-1 (%) | Reference |
|---------|--------------|--------|------------|-----------|-----------|-----------|
| ImageNet | ResNet-50 | 100 | 512 | LARS | 62.41 | 63.80 |
| ImageNet | ResNet-50 | 100 | 512 | AdamW | 60.08 | - |

## 5. License

This repositoy is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).
